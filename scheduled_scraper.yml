# .github/workflows/scheduled_scraper.yml

name: Scheduled Apartment Data Scraper

on:
  # This section defines the triggers for the workflow.
  schedule:
    # Run at 07:00 UTC (9:00 AM in CEST / Warsaw time)
    - cron: '0 7 * * *'

    # Run at 12:00 UTC (14:00 / 2:00 PM in CEST / Warsaw time)
    - cron: '0 12 * * *'

    # Run at 16:00 UTC (18:00 / 6:00 PM in CEST / Warsaw time)
    - cron: '0 16 * * *'

  # This allows you to also run the workflow manually from the GitHub Actions tab.
  workflow_dispatch:

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out your repository code
      - name: Check out repository
        uses: actions/checkout@v4

      # Step 2: Set up Python environment
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # Step 3: Install all required Python packages from your requirements.txt
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Run your Python scraper script
      - name: Run scraper.py
        run: python src/scraper.py

      # Step 5: Automatically commit and push the updated Excel file
      - name: Commit and push if data changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated data update: Refreshed apartment listings"
          file_pattern: "data/apartment/warsaw_private_owner_apartments.xlsx"
          commit_user_name: "GitHub Actions Bot"
          commit_user_email: "actions@github.com"
          commit_author: "GitHub Actions Bot <actions@github.com>"
